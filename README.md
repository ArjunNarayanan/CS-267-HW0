# Homework 0 - CS 267 - Applications of Parallel Computers - Arjun Narayanan
## My Bio
My academic and professional interests are broadly in the field of simulation and scientific computing. I developed an interest in this field during my masters in Civil and Environmental Engineering at Stanford University. I took courses on mechanics (solids and fluids), numerical methods (finite difference, explicit/implicit schemes), and the finite element method. For my research, I implemented an isogeometric finite element method which uses Bezier basis functions, and applied it to study high order PDEs of non-linear elasticity arising from higher-gradient formulations. Subsequently, I worked at Siemens Corporate Technology (2016-2017) as a research engineer. I used simulation to improve product design of various industrial machines, primarily for the Power & Gas division. I also implemented multi-scale analysis methods to aid with Design for Additive Manufacturing.

I am a PhD student in the department of Mechanical Engineering at UC Berkeley. I develop finite element methods to study solid-solid phase transformations that are relevant to geophysics. Through CS 267, I want to learn how I can reduce the compute time of my finite element simulations. I am also interested in computational geometry and would like to learn parallel algorithms that will be relevant to this field.

## Multi-physics phase field simulations
Phase field simulations are a popular class of numerical methods used to simulate microstructure evolution. These include transient phenomenon like solidification, dendrite growth, and phase transformations. (See Chen 2002 for an overview of the method.) At its heart, the phase field method aims to study the evolution of _order parameters_. Order parameters may be used as an indicator to represent, for example, various chemical species in a chemical reaction, differing phases of a material undergoing phase transition, damaged or fractured regions in a material that is failing. Often, it is necessary to couple these evolution equations with other physics -- for example heat transfer and mechanics.

### Multiphysics Object-Oriented Simulation Environment (MOOSE)
We will specifically examine the capabilities and application of the Multiphysics Object-Oriented Simulation Environment [MOOSE](https://mooseframework.inl.gov/) developed by the Idaho National Laboratory (INL). The core of the program is written largely in C++, with some use of Python (for postprocessing and user-interface). MOOSE is built on several well known and well tested numerical libraries including the finite element framework [libMesh](http://libmesh.github.io/) and the [PETSc](https://www.mcs.anl.gov/petsc/) and [Trilinos](https://github.com/trilinos/Trilinos) suites. Thus, MOOSE has been designed with scalability and parallelization in mind. MOOSE supports both shared memory (threading) and distributed memory (message passing) parallelism. Threading is achieved through Intel's Threading Building Blocks (TBB) and message passing is based on the Message Passing Interface (MPI). INL has developed several specialized modules for various classes of problems, including thermo-mechanics (BISON), geo-mechanics (FALCON), phase-field (MARMOT) among others. (For a more detailed list please see [this presentation](https://mooseframework.inl.gov/static/media/uploads/docs/main.pdf).) INL users regularly perform large scale simulations using MOOSE on their supercomputer FALCON which ranks 361 on the top500 list as of 2018.

### Parallel finite element and phase field simulations using MOOSE
Gaston et al 2013 provide an overview of the "hybrid" parallel design of MOOSE. By "hybrid" they mean that the program is simultaneously capable of distributed memory parallelism and shared memory parallelism. The paper provides example computations that were carried out on the Fission supercomputer of Idaho National Laboratory. As of 2011, Fission consisted of 391 computing nodes each with four 8-core 2.4 GHz AMD Opteron 6136 processors. Gaston et al 2013 go on to discuss a test problem which involves a tightly coupled set of nonlinear PDEs describing nuclear fuel life-cycle. Their final system of equations consisted of 234M degrees of freedom running on over 12,000 cores of the Fission supercomputer. The parallel scalability study indicates that MOOSE can deliver up to 80% relative efficiency.

Tonks et al 2012 discuss the phase field framework within MOOSE. In particular they discuss some aspects of the parallel capabilities of MOOSE. They claim that the program is "fully parallel... without requiring MPI calls or any other parallel considerations". To demonstrate parallel scalability, the authors consider a model phase-separation problem. They use a constant spatial discretization with 161,604 degrees of freedom, and 10 time steps. The problem is solved on between 1 to 12 Intel Nehalem processors (2.93 GHz). They report ideal scaling until about 20,000 degrees of freedom per processor.

### Application: Temperature dependent stability of Al-Cu alloys
In Shower et al 2018, phase field simulations were performed using the MOOSE framework to understand the stability regimes of precipitate strengthened Al-Cu alloys. Commercial Al-Cu alloys are strengthened by the presence of high aspect ratio disc shaped precipitates of Al<sub>2</sub>Cu. At elevated temperatures, phase transformations cause these precipitates to grow, with a reduction in aspect ratio. At a critical aspect ratio of the precipitate, there is a significant deterioration (~60%) in strength and hardness of the alloy. The time taken to reach this critical aspect ratio is affected by (a) interfacial thermodynamics (b) diffusion kinetics. Shower et al 2018 use the phase field method to quantify this effect and produce plots showing the _stability regime_ of various modified alloys in which the thermodynamics and kinetics have been modified. 

There is a general agreement of their numerical results with experimental data. However, some limitations prevent a more precise comparison. For example, they perform simplified 2D simulations for a problem that is inherently 3D. This choice was likely influenced by the large computational cost of phase field methods. Phase field simulations are characterized by a length scale that is on the order of the interface length scale. Further, being a transient phenomenon, it is often necessary to have small time steps during periods of rapid microstructure evolution. These challenges are overcome to some extent by Shower et al by taking advantage of spatial mesh adaptivity and adaptive time stepping which is made available through MOOSE. However, they do not mention if their simulations were performed in parallel, even though MOOSE provides extensive support for large scale parallelization. More accurate simulations are likely to be achieved by utilizing this capability of the software.



## Bibliography
Chen, Long-Qing. "Phase-field models for microstructure evolution." Annual review of materials research 32, no. 1 (2002): 113-140.

Tonks, Michael R., Derek Gaston, Paul C. Millett, David Andrs, and Paul Talbot. "An object-oriented finite element framework for multiphysics phase field simulations." Computational Materials Science 51, no. 1 (2012): 20-29.

Shower, Patrick, James R. Morris, Dongwon Shin, Balasubramaniam Radhakrishnan, Lawrence L. Allard, and Amit Shyam. "Temperature-dependent stability of θ′-Al2Cu precipitates investigated with Phase Field simulations and experiments." Materialia (2018): 100185.

Gaston, Derek R., Cody J. Permann, David Andrs, and John W. Peterson. "Massive hybrid parallelism for fully implicit multiphysics." In Proceedings of the International Conference on Mathematics and Computational Methods Applied to Nuclear Science & Engineering (M&C 2013).< http://www. inl. gov/technicalpublications/Documents/5864439. pdf. 2013.
